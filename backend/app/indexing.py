from pathlib import Path
from typing import Iterator, List
from dataclasses import dataclass
import ast

# –ü—É—Ç—å –¥–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ (.../HaHaTone228_PAW_AI)
PROJECT_ROOT = Path(__file__).resolve().parents[2]
# –ü—É—Ç—å –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è reddit
REDDIT_ROOT = PROJECT_ROOT / "data" / "reddit"

TEXT_EXTS = (".json", ".md", ".txt", ".ini", ".cfg", ".yaml", ".yml")




@dataclass
class CodeChunk:
    """
    –û–¥–∏–Ω –ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ (—Ñ—É–Ω–∫—Ü–∏—è –∏–ª–∏ –∫–ª–∞—Å—Å).
    """
    file_path: str   # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ reddit_repo
    start_line: int
    end_line: int
    kind: str        # "function" / "class" / "text" / "config" / "doc"
    name: str        # –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏/–∫–ª–∞—Å—Å–∞ –∏–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞
    code: str        # —Ç–µ–∫—Å—Ç –∫–æ–¥–∞/–¥–æ–∫—É–º–µ–Ω—Ç–∞
    language: str = "python"  # "python", "json", "markdown", "text", "config", ...

def iter_code_files(root: Path, exts=(".py",)) -> Iterator[Path]:
    """
    –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º —Å –Ω—É–∂–Ω—ã–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ root.
    –°–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å .py —Ñ–∞–π–ª–∞–º–∏.
    """
    for path in root.rglob("*"):
        if path.is_file() and path.suffix in exts:
            yield path


def iter_text_files(root: Path, exts=TEXT_EXTS) -> Iterator[Path]:
    """
    –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–∞–º (json, md, txt, –∫–æ–Ω—Ñ–∏–≥–∏).
    """
    for path in root.rglob("*"):
        if path.is_file() and path.suffix.lower() in exts:
            yield path

def _detect_language_from_suffix(path: Path) -> str:
    suf = path.suffix.lower()
    if suf == ".json":
        return "json"
    if suf in (".md", ".markdown"):
        return "markdown"
    if suf in (".yaml", ".yml"):
        return "yaml"
    if suf in (".ini", ".cfg"):
        return "config"
    if suf == ".txt":
        return "text"
    return "text"


def extract_chunks_from_text_file(path: Path) -> List[CodeChunk]:
    """
    –î–æ—Å—Ç–∞—ë—Ç —á–∞–Ω–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (json, md, txt, –∫–æ–Ω—Ñ–∏–≥–∏).
    –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: –æ–¥–∏–Ω —á–∞–Ω–∫ = –≤–µ—Å—å —Ñ–∞–π–ª.
    –ï—Å–ª–∏ —Ñ–∞–π–ª –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π, –ø—Ä–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–µ –º—ã –≤—Å—ë —Ä–∞–≤–Ω–æ –µ–≥–æ —É—Ä–µ–∂–µ–º.
    """
    try:
        text = path.read_text(encoding="utf-8", errors="ignore")
    except OSError:
        return []

    lines = text.splitlines()
    language = _detect_language_from_suffix(path)
    rel_path = str(path.relative_to(REDDIT_ROOT))

    chunk = CodeChunk(
        file_path=rel_path,
        start_line=1,
        end_line=len(lines) if lines else 1,
        kind="text",
        name=path.name,
        code=text,
        language=language,
    )

    return [chunk]


def extract_chunks_from_python_file(path: Path) -> List[CodeChunk]:
    """
    –†–∞–∑–±–∏—Ä–∞–µ—Ç .py —Ñ–∞–π–ª —á–µ—Ä–µ–∑ ast –∏ –¥–æ—Å—Ç–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã.
    –í —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º:
    - –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏/–∫–ª–∞—Å—Å–∞,
    - docstring (–µ—Å–ª–∏ –µ—Å—Ç—å),
    - —Å–∞–º –∫–æ–¥.
    """
    text = path.read_text(encoding="utf-8", errors="ignore")
    try:
        tree = ast.parse(text)
    except SyntaxError:
        # –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –º–æ–≥—É—Ç –Ω–µ –ø–∞—Ä—Å–∏—Ç—å—Å—è ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        return []

    lines = text.splitlines()
    chunks: List[CodeChunk] = []

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            start = node.lineno
            end = getattr(node, "end_lineno", None) or start

            # –∑–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã
            start = max(start, 1)
            end = min(end, len(lines))

            code_lines = lines[start - 1:end]
            code = "\n".join(code_lines)

            kind = "class" if isinstance(node, ast.ClassDef) else "function"

            # docstring, –µ—Å–ª–∏ –µ—Å—Ç—å
            docstring = ast.get_docstring(node)
            header_parts = [f"# {kind} {node.name}"]
            if docstring:
                header_parts.append(f"# doc: {docstring.strip().replace('\n', ' ')}")
            header_text = "\n".join(header_parts)

            full_text = f"{header_text}\n\n{code}"

            chunks.append(
                CodeChunk(
                    file_path=str(path.relative_to(REDDIT_ROOT)),
                    start_line=start,
                    end_line=end,
                    kind=kind,
                    name=node.name,
                    code=full_text,
                    language="python",
                )
            )

    return chunks


def collect_all_chunks(limit_files: int | None = None) -> List[CodeChunk]:
    """
    –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º .py —Ñ–∞–π–ª–∞–º –≤ reddit_repo –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —á–∞–Ω–∫–∏,
    –∞ —Ç–∞–∫–∂–µ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–∞–º (json, md, txt, –∫–æ–Ω—Ñ–∏–≥–∏).
    limit_files ‚Äî –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–ø–æ –∫–æ–¥—É –∏ –ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–¥–µ–ª—å–Ω–æ).
    """
    if not REDDIT_ROOT.exists():
        print(f"reddit_repo not found: {REDDIT_ROOT}")
        return []

    all_chunks: List[CodeChunk] = []
    total_py_files = 0
    py_files_with_chunks = 0

    # 1) Python-—Ñ–∞–π–ª—ã
    for i, path in enumerate(iter_code_files(REDDIT_ROOT)):
        if limit_files is not None and i >= limit_files:
            break

        total_py_files += 1
        chunks = extract_chunks_from_python_file(path)
        if chunks:
            py_files_with_chunks += 1
        all_chunks.extend(chunks)

    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ .py –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_py_files}")
    print(f"–§–∞–π–ª–æ–≤, –≥–¥–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏/–∫–ª–∞—Å—Å—ã: {py_files_with_chunks}")
    print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ (—Ñ—É–Ω–∫—Ü–∏–π/–∫–ª–∞—Å—Å–æ–≤): {len(all_chunks)}")

    # 2) –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (json, md, txt, –∫–æ–Ω—Ñ–∏–≥–∏)
    total_text_files = 0
    text_files_with_chunks = 0

    for j, path in enumerate(iter_text_files(REDDIT_ROOT)):
        if limit_files is not None and j >= limit_files:
            break

        total_text_files += 1
        chunks = extract_chunks_from_text_file(path)
        if chunks:
            text_files_with_chunks += 1
        all_chunks.extend(chunks)

    print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (json/md/txt/config) –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_text_files}")
    print(f"–§–∞–π–ª–æ–≤, –≥–¥–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏: {text_files_with_chunks}")
    print(f"–ò–¢–û–ì–û –í–°–ï–• –ß–ê–ù–ö–û–í (–∫–æ–¥ + —Ç–µ–∫—Å—Ç): {len(all_chunks)}")

    return all_chunks


def debug_one_file_with_chunks():
    """
    –ù–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π .py —Ñ–∞–π–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å —á–∞–Ω–∫–∏, –∏ –≤—ã–≤–µ—Å—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ.
    """
    if not REDDIT_ROOT.exists():
        print(f"reddit_repo not found: {REDDIT_ROOT}")
        return

    for path in iter_code_files(REDDIT_ROOT):
        chunks = extract_chunks_from_python_file(path)
        if not chunks:
            continue

        print(f"FILE: {path}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\n")

        for ch in chunks[:5]:
            print("-" * 60)
            print(f"{ch.kind.upper()} {ch.name} ({ch.start_line}-{ch.end_line})")
            print(ch.code)
            print()
        break
    else:
        print("–í–æ–æ–±—â–µ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏/–∫–ª–∞—Å—Å–∞–º–∏ ü§î")




def build_reddit_index():
    """
    –°–æ–±—Ä–∞—Ç—å –≤—Å–µ python-—á–∞–Ω–∫–∏ –∏ –∑–∞–ø–∏—Å–∞—Ç—å –∏—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É.
    """
    #–ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–µ–ª–∞–µ–º
    from .vector_store import build_index  # –∏–º–ø–æ—Ä—Ç –≤ –∫–æ–Ω—Ü–µ, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Ü–∏–∫–ª–æ–≤

    chunks = collect_all_chunks()
    if not chunks:
        print("–ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—Ç—Ä–æ–∏–º")
        return

    print("–ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
    build_index(chunks)
    print("–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω.")


if __name__ == "__main__":
    
    
    build_reddit_index()

    #–î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å:
    # collect_all_chunks()
    # debug_one_file_with_chunks()

    # –î–ª—è –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º:
    #collect_all_chunks()

    # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–µ—Ä–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ –∫–∞–∫–æ–≥–æ-–Ω–∏–±—É–¥—å —Ñ–∞–π–ª–∞:
    # debug_one_file_with_chunks()